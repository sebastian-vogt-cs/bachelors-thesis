\section[RQ2]{RQ2 - Is there a per-objective statistical difference regarding neutrality and ruggedness of the fitness landscape generated by the two fitness functions?}

\subsection{Analysis}

For the statistical comparison between the two fitness functions we devise a test modelled after the guidelines proposed by Arcuri and Briand \cite{arcuri2011practical}.
The explained procedure is done for every fitness landscape measure, but for simplicity, the following explanation just speaks of \emph{the measure}.
For autocorrelation, $k=1$ was used to conduct the statistical tests.

As exemplified in table \ref{table:walks}, we have a list of observations, where each one belongs to one of our two fitness functions, to a certain objective (a branch of an app) and has a certain measure value.
For every objective and fitness function, we have 20 measure values.
The goal is now to compare the two fitness functions for statistical difference in the measure values for each objective.
Since the fitness landscape measures are interval-scale results, we use the (two-sided) Mann-Whitney U-test \cite{mann1947test} to test for statistical difference between the measures of the two fitness functions, which results in a p-value for every objective.
We summarize their distribution in a histogram.
We also visualize the number of branches with $p < 0.05$ and $p \geq 0.05$ in a bar chart.

For all objectives, the effect size by Vargha and Delaney \cite{vargha2000critique} is calculated.
In our case, the effect size represents the probability, that code-based fitness results in higher measure values than branch distance \cite{arcuri2011practical}.
Their distribution across all objectives is represented in a violin plot for both $p < 0.05$ and $p \geq 0.05$.
Since the result is a distribution of a large amount of effect size values, we refrain from calculating the confidence interval for each of those values. 
Additional representations of the data like bar-charts are added based on the received distributions.
Further information on the respective distributions of measure values is not presented, as this would result in an unwieldy amount of data.

\subsection{Results}

\begin{figure}[h]
	\includegraphics[width = 0.5\textwidth]{../jupyter/out/neutrality_distance_p-values.pdf}
	\includegraphics[width = 0.5\textwidth]{../jupyter/out/neutrality_distance_p-values_two-bins.pdf}
	%
	\includegraphics[width = 0.5\textwidth]{../jupyter/out/neutrality_volume_p-values.pdf}
	\includegraphics[width = 0.5\textwidth]{../jupyter/out/neutrality_volume_p-values_two-bins.pdf}
	\caption{Distribution of p-values for neutrality distance and neutrality volume}\label{fig:pNeutral}
\end{figure}

For every measure, the p-values of the Mann-Whitney U-test are represented in a histogram with bin-width $0.05$ and in a bar diagram with one bar for $p < 0.05$ and one for $p \geq 0.05$.
The p-values for neutrality distance and neutrality volume can be found in figure \ref{fig:pNeutral}.
For neutrality distance, $1037$ out of $1300$ branches resulted in a p-value smaller than $0.05$.
For neutrality volume, $1222$ fell into that category.
The p-values for information content and autocorrelation can be found in figure \ref{fig:bug}, where $1228$ branches had a p-value below $0.05$ for information content and $734$ for autocorrelation. (For autocorrelation, only $1045$ branches are included, because the other branches had only flat walks and thus no autocorrelation values for code-based fitness.)

The Vargha and Delaney effect size values for all measures can be found in figure \ref{fig:us}.
There is a violin plot for those objectives with $p < 0.05$ and one for $p \geq 0.05$.
For neutrality volume and information content, a bar plot showing the amount of effect sizes in the intervals $[0, 0]$, $(0, 0.5)$ and $(0.5, 1]$ among the branches with $p < 0.05$ can be seen.

\begin{figure}[h]
	\includegraphics[width = 0.5\textwidth]{../jupyter/out/information_content_p-values.pdf}
	\includegraphics[width = 0.5\textwidth]{../jupyter/out/information_content_p-values_two-bins.pdf}
	%
	\includegraphics[width = 0.5\textwidth]{../jupyter/out/autocorrelation_p-values.pdf}
	\includegraphics[width = 0.5\textwidth]{../jupyter/out/autocorrelation_p-values_two-bins.pdf}
	\caption{Distribution of p-values for information content and autocorrelation}\label{fig:bug}
\end{figure}

\begin{figure}[h]
	\includegraphics[width = 0.5\textwidth]{../jupyter/out/neutrality_distance_effect_size.pdf}
	%
	\includegraphics[width = 0.5\textwidth]{../jupyter/out/neutrality_volume_effect_size.pdf}
	\includegraphics[width = 0.5\textwidth]{../jupyter/out/neutrality_volume_effect_size_two-bins.pdf}
	%
	%
	\includegraphics[width = 0.5\textwidth]{../jupyter/out/information_content_effect_size.pdf}
	\includegraphics[width = 0.5\textwidth]{../jupyter/out/information_content_effect_size_two-bins.pdf}
	%
	\includegraphics[width = 0.5\textwidth]{../jupyter/out/autocorrelation_effect_size.pdf}
	\caption{Distribution of effect sizes for neutrality distance, neutrality volume, information content and autocorrelation}\label{fig:us}
\end{figure}

\subsection{Discussion}

Figures \ref{fig:pNeutral} and \ref{fig:bug} show that for all measures, the majority of branches result in a p-value smaller than $0.05$. For neutrality volume and information content, almost all branches fall into that category, while for neutrality distance and especially autocorrelation, many branches have a higher p-value.
This shows us that the number of $20$ repetitions was enough to generate significant results, although more repetitions could have further improved the results.

For neutrality distance, the largest peak in the distribution of the effect size values is at nearly $1.0$.
This means that for many branches, code-based fitness is almost bound to generate higher neutrality distance values than branch distance.
For some branches on the other hand, this effect is less pronounced, or even opposite.
Unsurprisingly, for $p \geq 0.05$, most effect sizes are close to $0.5$.

Regarding neutrality volume, more than two thirds of the objectives have an effect size of $0$, which means that for these objectives, code-based fitness is bound to generate lower neutrality volume values than branch distance.
The data statistically supports our observations from RQ1, that the fitness landscape of code-based fitness is more neutral then the landscape of branch distance.

For the information content measure, the distribution of effect sizes is almost indiscernible from the one for neutrality volume.
This means that for the majority of branches, code-based fitness generates statistically significantly lower information content values than branch distance.
This statistically supports our observations from RQ1, that the fitness landscape  of code-based fitness has a lower information content.

In the violin plot for autocorrelation, it is visible that all branches with $p < 0.05$ and almost all branch with $p \geq 0.05$ have a statistically significantly lower autocorrelation with code-based fitness, which again reflects the observation from RQ1.

The similarity of the violin plots between information content and the neutrality volume strengthens the suspicion raised in RQ1, that the effects observed regarding the ruggedness measures are mainly caused by the neutrality of the fitness landscape of code-based fitness.


\begin{mdframed}[style=box, frametitle={Summary RQ2:}]
	For the majority of branches, there is a statistically significant difference in measure values, which supports the results from RQ1.
	Further evidence is raised that the results of the ruggedness measures are influenced by the higher neutrality of code-based fitness rather than ruggedness itself.
\end{mdframed}